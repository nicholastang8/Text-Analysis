{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLc-Y-xRRU-7"
      },
      "source": [
        "Data informatiion: Data 1,2,3 & Import data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NdOeXBv-jU59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKZa0BZ_RUjK",
        "outputId": "f990f31f-b839-4b0a-c4b8-61b8610dffd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.\n",
            "The big black dog barked at the white cat and chased away.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import RegexpStemmer,PorterStemmer, LancasterStemmer\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "# Preprocess the tokenized text for 3-grams language modelling\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE # Maximum Likelihood Estimation\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "data1 = \"Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties.\"\n",
        "print(data1)\n",
        "\n",
        "data_2 = \"The big black dog barked at the white cat and chased away.\"\n",
        "print(data_2)\n",
        "#tokenizing the data\n",
        "text_corpus = word_tokenize(data1)\n",
        "data_2_tokenized = word_tokenize(data_2)\n",
        "\n",
        "data_2_textBlob = TextBlob(data_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-o0JmNch2dg"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAoqWgSWcLJp"
      },
      "source": [
        "1.\tDemonstrate word tokenisation using the split function, Regular Expression and NLTK packages separately and report the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZatEpxpF5Ag",
        "outputId": "00fb78c3-5734-4cb8-d5d1-a6f19a4e0b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split Function:\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into']\n",
            "['two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions']\n",
            "['about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective']\n",
            "['expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,']\n",
            "['events,', 'and', 'their', 'properties.']\n"
          ]
        }
      ],
      "source": [
        "#split function\n",
        "token_split = data1.split()\n",
        "\n",
        "print(\"Split Function:\")\n",
        "for i in range(0, len(token_split), 10):\n",
        "    print(token_split[i:i+10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTum8OYlpRtW",
        "outputId": "cabcdb94-6824-4b66-d88b-b3cfa8859b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular Expression:\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into']\n",
            "['two', 'main', 'types', 'facts', 'and', 'opinions', 'Facts', 'are', 'objective', 'expressions']\n",
            "['about', 'entities', 'events', 'and', 'their', 'properties', 'Opinions', 'are', 'usually', 'subjective']\n",
            "['expressions', 'that', 'describe', 'people', 's', 'sentiments', 'appraisals', 'or', 'feelings', 'toward']\n",
            "['entities', 'events', 'and', 'their', 'properties']\n"
          ]
        }
      ],
      "source": [
        "#regular expression\n",
        "token_reg = re.findall(r'\\b\\w+\\b',data1)\n",
        "\n",
        "print(\"Regular Expression:\")\n",
        "for i in range(0, len(token_reg), 10):\n",
        "    print(token_reg[i:i+10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLTK\n",
        "tokens = word_tokenize(data1)\n",
        "print(\"NLTK:\")\n",
        "for i in range(0, len(tokens), 10):\n",
        "    print(tokens[i:i+10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAzhEtm9bim4",
        "outputId": "5199bc0c-4f32-4952-b456-df142fde077a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK:\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into']\n",
            "['two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are']\n",
            "['objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties']\n",
            "['.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’']\n",
            "['s', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',']\n",
            "['events', ',', 'and', 'their', 'properties', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcKOYbYKhafK"
      },
      "source": [
        "3. Demonstrate stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def remove_stopwords_nltk(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def remove_punctuations_regex(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def remove_stopwords_textblob(text):\n",
        "    blob = TextBlob(text)\n",
        "    filtered_words = [word for word in blob.words if word.lower() not in stopwords.words(\"english\")]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "# Remove stop words using NLTK\n",
        "result_nltk = remove_stopwords_nltk(data1)\n",
        "print(\"NLTK Output:\")\n",
        "for i in range(0, len(result_nltk.split()), 10):\n",
        "    print(' '.join(result_nltk.split()[i:i+10]))\n",
        "print(\"\\nStop Words in NLTK:\")\n",
        "print([word for word in nltk.word_tokenize(data1) if word.lower() in stop_words])\n",
        "\n",
        "# Remove stop words using TextBlob\n",
        "result_textblob = remove_stopwords_textblob(data1)\n",
        "print(\"\\nTextBlob Output:\")\n",
        "for i in range(0, len(result_textblob.split()), 10):\n",
        "    print(' '.join(result_textblob.split()[i:i+10]))\n",
        "print(\"\\nStop Words in TextBlob:\")\n",
        "print([word for word in TextBlob(data1).words if word.lower() in stop_words])\n",
        "\n",
        "# Remove punctuations using regex\n",
        "result_regex = remove_punctuations_regex(data1)\n",
        "print(\"\\nRegex Output:\")\n",
        "for i in range(0, len(result_regex.split()), 10):\n",
        "    print(' '.join(result_regex.split()[i:i+10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOD0BcwCy2UW",
        "outputId": "b28c343e-d3f1-4287-d6ad-80854fbca9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Output:\n",
            "Textual information world broadly categorized two main types : facts\n",
            "opinions . Facts objective expressions entities , events , properties\n",
            ". Opinions usually subjective expressions describe people ’ sentiments ,\n",
            "appraisals , feelings toward entities , events , properties .\n",
            "\n",
            "Stop Words in NLTK:\n",
            "['in', 'the', 'can', 'be', 'into', 'and', 'are', 'about', 'and', 'their', 'are', 'that', 's', 'or', 'and', 'their']\n",
            "\n",
            "TextBlob Output:\n",
            "Textual information world broadly categorized two main types facts opinions\n",
            "Facts objective expressions entities events properties Opinions usually subjective expressions\n",
            "describe people ’ sentiments appraisals feelings toward entities events properties\n",
            "\n",
            "Stop Words in TextBlob:\n",
            "['in', 'the', 'can', 'be', 'into', 'and', 'are', 'about', 'and', 'their', 'are', 'that', 's', 'or', 'and', 'their']\n",
            "\n",
            "Regex Output:\n",
            "Textual information in the world can be broadly categorized into\n",
            "two main types facts and opinions Facts are objective expressions\n",
            "about entities events and their properties Opinions are usually subjective\n",
            "expressions that describe peoples sentiments appraisals or feelings toward entities\n",
            "events and their properties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSmRVwwbieJc"
      },
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yw_qND-ig3s"
      },
      "source": [
        "2.\tDemonstrate word stemming using Regular Expression, Porter Stemmer and Lancaster Stemmer and report the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBuZRqMQMJlw",
        "outputId": "faf6aed3-9b1d-49e8-fbad-0ddb616964ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular Expression Output:\n",
            "['Text', 'inform', 'in', 'the', 'world', 'can', 'be', 'broad', 'categoriz', 'into']\n",
            "['two', 'main', 'type', ':', 'fact', 'and', 'opinion', '.', 'Fact', 'are']\n",
            "['object', 'expression', 'about', 'entitie', ',', 'event', ',', 'and', 'their', 'propertie']\n",
            "['.', 'Opinion', 'are', 'usual', 'subject', 'expression', 'that', 'describe', 'people', '’']\n",
            "['s', 'sentiment', ',', 'appraisal', ',', 'or', 'feeling', 'toward', 'entitie', ',']\n",
            "['event', ',', 'and', 'their', 'propertie', '.']\n",
            "\n",
            "Porter Stemmer Output:\n",
            "['textual', 'inform', 'in', 'the', 'world', 'can', 'be', 'broadli', 'categor', 'into']\n",
            "['two', 'main', 'type', ':', 'fact', 'and', 'opinion', '.', 'fact', 'are']\n",
            "['object', 'express', 'about', 'entiti', ',', 'event', ',', 'and', 'their', 'properti']\n",
            "['.', 'opinion', 'are', 'usual', 'subject', 'express', 'that', 'describ', 'peopl', '’']\n",
            "['s', 'sentiment', ',', 'apprais', ',', 'or', 'feel', 'toward', 'entiti', ',']\n",
            "['event', ',', 'and', 'their', 'properti', '.']\n",
            "\n",
            "Lancaster Stemmer Output:\n",
            "['text', 'inform', 'in', 'the', 'world', 'can', 'be', 'broad', 'categ', 'into']\n",
            "['two', 'main', 'typ', ':', 'fact', 'and', 'opin', '.', 'fact', 'ar']\n",
            "['object', 'express', 'about', 'ent', ',', 'ev', ',', 'and', 'their', 'property']\n",
            "['.', 'opin', 'ar', 'us', 'subject', 'express', 'that', 'describ', 'peopl', '’']\n",
            "['s', 'senty', ',', 'appra', ',', 'or', 'feel', 'toward', 'ent', ',']\n",
            "['ev', ',', 'and', 'their', 'property', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Stemming using Regular Expression\n",
        "rs = RegexpStemmer('ing$|able$|ed$|ly$|er$|en$|ish$|ment$|ous$|ion$|al$|ive$|ify$|ful$|less$|ness$|ity$|ual$|ation$|s$', min=3)\n",
        "print(\"Regular Expression Output:\")\n",
        "rs_stem = [rs.stem(w) for w in text_corpus]\n",
        "for i in range(0, len(rs_stem), 10):\n",
        "    print(rs_stem[i:i+10])\n",
        "\n",
        "#stemming using Porter Stemmer\n",
        "ps = PorterStemmer()\n",
        "print(\"\\nPorter Stemmer Output:\")\n",
        "ps_stem = [ps.stem(w) for w in text_corpus]\n",
        "for i in range(0, len(ps_stem), 10):\n",
        "    print(ps_stem[i:i+10])\n",
        "\n",
        "# Stemming using Lancaster Stemmer\n",
        "ls = LancasterStemmer()\n",
        "print(\"\\nLancaster Stemmer Output:\")\n",
        "ls_stem = [ls.stem(w) for w in text_corpus]\n",
        "for i in range(0, len(ls_stem), 10):\n",
        "    print(ls_stem[i:i+10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGOA4VmV_YAw"
      },
      "source": [
        "# Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJwH0k-k_d0s"
      },
      "source": [
        "1.\tDemonstrate POS tagging using NLTK POS tagger, textblob POS tagger and the Regular Expression tagger and report the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dDb_HSlApX0",
        "outputId": "18c35437-8ee9-4e5e-bf82-71d0c2600634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK POS tagger output:\n",
            "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD')]\n",
            "[('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC')]\n",
            "[('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Textblob POS tagger output:\n",
            "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD')]\n",
            "[('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC')]\n",
            "[('chased', 'VBD'), ('away', 'RB')]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Regular Expression tagger output:\n",
            "[('The', 'NN'), ('big', 'NN'), ('black', 'NN'), ('dog', 'NN'), ('barked', 'VBD')]\n",
            "[('at', 'NN'), ('the', 'DT'), ('white', 'NN'), ('cat', 'NN'), ('and', 'CC')]\n",
            "[('chased', 'VBD'), ('away', 'NN'), ('.', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "# NLTK POS tagger\n",
        "print(\"NLTK POS tagger output:\")\n",
        "nltk_pos = nltk.pos_tag(data_2_tokenized)\n",
        "for i in range(0, len(nltk_pos), 5):\n",
        "    print(nltk_pos[i:i+5])\n",
        "print(\"-\" * 200)\n",
        "\n",
        "# textblob POS tagger\n",
        "print(\"Textblob POS tagger output:\")\n",
        "textblob_pos = data_2_textBlob.tags\n",
        "for i in range(0, len(textblob_pos), 5):\n",
        "    print(textblob_pos[i:i+5])\n",
        "\n",
        "# Regular Expression tagger\n",
        "patterns = [\n",
        "     (r'.*ing$', 'VBG'),               # gerunds\n",
        "     (r'.*ed$', 'VBD'),                # simple past\n",
        "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
        "     (r'.*ould$', 'MD'),               # modals\n",
        "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
        "     (r'.*s$', 'NNS'),                 # plural nouns\n",
        "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
        "     (r'.*the$', 'DT'),                # the\n",
        "     (r'.*and$', 'CC'),                # and\n",
        "     (r'.*', 'NN'),                    # nouns (default)\n",
        "     (r'^\\d+$', 'CD'),\n",
        "     (r'.*ing$', 'VBG'),               # gerunds, i.e. wondering\n",
        "     (r'.*ment$', 'NN'),               # i.e. wonderment\n",
        "     (r'.*ful$', 'JJ')                 # i.e. wonderful\n",
        " ]\n",
        "\n",
        "regexp_tagger = nltk.RegexpTagger(patterns)\n",
        "tagger = nltk.tag.sequential.RegexpTagger(patterns)\n",
        "print(\"-\" * 200)\n",
        "print(\"Regular Expression tagger output:\")\n",
        "reg_pos = tagger.tag(data_2_tokenized)\n",
        "for i in range(0, len(reg_pos), 5):\n",
        "    print(reg_pos[i:i+5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "id": "kbnAbCeHDVN5",
        "outputId": "d5b106f0-1c27-4304-a3f7-c48339ee4ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: svgling in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.10/dist-packages (from svgling) (1.4.3)\n",
            "['the', 'big', 'black', 'dog', 'barked', 'at', 'the', 'white', 'cat', 'and', 'chased', 'away']\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"408px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,560.0,408.0\" width=\"560px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"31.4286%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"22.7273%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.3636%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"77.2727%\" x=\"22.7273%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOM</text></svg><svg width=\"29.4118%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">big</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"41.1765%\" x=\"29.4118%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">black</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"29.4118%\" x=\"70.5882%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.2941%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.3636%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.7143%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"68.5714%\" x=\"31.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"16.6667%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">barked</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.33333%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"83.3333%\" x=\"16.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"10%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"90%\" x=\"10%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"13.8889%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.94444%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"86.1111%\" x=\"13.8889%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOM</text></svg><svg width=\"22.5806%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">white</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.2903%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"16.129%\" x=\"22.5806%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">cat</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.6452%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"61.2903%\" x=\"38.7097%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOM</text></svg><svg width=\"26.3158%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.1579%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"73.6842%\" x=\"26.3158%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"57.1429%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">chased</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.5714%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"42.8571%\" x=\"57.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADV</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">away</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.5714%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.1579%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.3548%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.9444%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.3333%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.7143%\" y1=\"19.2px\" y2=\"48px\" /></svg>",
            "text/plain": [
              "Tree('S', [Tree('NP', [Tree('DT', ['the']), Tree('NOM', [Tree('ADJ', ['big']), Tree('ADJ', ['black']), Tree('NN', ['dog'])])]), Tree('VP', [Tree('V', ['barked']), Tree('PP', [Tree('IN', ['at']), Tree('NP', [Tree('DT', ['the']), Tree('NOM', [Tree('ADJ', ['white']), Tree('NN', ['cat']), Tree('NOM', [Tree('CJ', ['and']), Tree('VP', [Tree('V', ['chased']), Tree('ADV', ['away'])])])])])])])])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"408px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,560.0,408.0\" width=\"560px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"31.4286%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"22.7273%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.3636%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"77.2727%\" x=\"22.7273%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOM</text></svg><svg width=\"29.4118%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">big</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.7059%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"70.5882%\" x=\"29.4118%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOM</text></svg><svg width=\"58.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">black</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.1667%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"41.6667%\" x=\"58.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dog</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.1667%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.7059%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"61.3636%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.7143%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"68.5714%\" x=\"31.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"16.6667%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">barked</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.33333%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"83.3333%\" x=\"16.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PP</text></svg><svg width=\"10%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"90%\" x=\"10%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"13.8889%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.94444%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"86.1111%\" x=\"13.8889%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOM</text></svg><svg width=\"22.5806%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">white</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.2903%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"16.129%\" x=\"22.5806%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">cat</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.6452%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"61.2903%\" x=\"38.7097%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOM</text></svg><svg width=\"26.3158%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.1579%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"73.6842%\" x=\"26.3158%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VP</text></svg><svg width=\"57.1429%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">chased</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.5714%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"42.8571%\" x=\"57.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADV</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">away</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.5714%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.1579%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.3548%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.9444%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.3333%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.7143%\" y1=\"19.2px\" y2=\"48px\" /></svg>",
            "text/plain": [
              "Tree('S', [Tree('NP', [Tree('DT', ['the']), Tree('NOM', [Tree('ADJ', ['big']), Tree('NOM', [Tree('ADJ', ['black']), Tree('NN', ['dog'])])])]), Tree('VP', [Tree('V', ['barked']), Tree('PP', [Tree('IN', ['at']), Tree('NP', [Tree('DT', ['the']), Tree('NOM', [Tree('ADJ', ['white']), Tree('NN', ['cat']), Tree('NOM', [Tree('CJ', ['and']), Tree('VP', [Tree('V', ['chased']), Tree('ADV', ['away'])])])])])])])])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import nltk\n",
        "!pip install svgling\n",
        "from nltk import Tree\n",
        "\n",
        "# Download the necessary resources (if not already downloaded)\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "NP -> DT NOM\n",
        "NOM -> ADJ NOM | ADJ NN | ADJ ADJ NN | ADJ NN NOM | CJ VP\n",
        "PP -> IN NP\n",
        "VP -> V PP | V ADV\n",
        "\n",
        "V -> 'barked' | 'chased'\n",
        "DT -> 'the'\n",
        "ADJ -> 'big' | 'black' | 'white'\n",
        "NN -> 'dog' | 'cat'\n",
        "IN -> 'at'\n",
        "CJ -> 'and'\n",
        "ADV -> 'away'\n",
        "\"\"\")\n",
        "\n",
        "data3 = nltk.tokenize.word_tokenize(\"the big black dog barked at the white cat and chased away\")\n",
        "print(data3)\n",
        "print()\n",
        "parser = nltk.ChartParser(grammar)\n",
        "for tree1 in parser.parse(data3):\n",
        "  display(tree1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZzHXvYodV6t"
      },
      "source": [
        "#Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAWkHPdNdX4C"
      },
      "source": [
        "[link text](https://)3.\tImplement in python using both unsmoothed and smoothed bigram language models and report the respective sentence probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsmoothed Bigram Model"
      ],
      "metadata": {
        "id": "jSpvwCcmijcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import bigrams\n",
        "from collections import Counter\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"He read a book\",\n",
        "    \"I read a different book\",\n",
        "    \"He read a book by Danielle\"\n",
        "]\n",
        "\n",
        "# Tokenize each sentence and generate bigrams\n",
        "corpus_bigrams = []\n",
        "corpus_unigrams = []\n",
        "for sentence in sentences:\n",
        "    # Tokenize the sentence\n",
        "    tokens = nltk.tokenize.word_tokenize(sentence.lower())\n",
        "\n",
        "    # Add padding to the tokens\n",
        "    padded_tokens = ['<s>'] + tokens + ['</s>']\n",
        "\n",
        "    # Generate bigrams\n",
        "    bigram_tokens = list(bigrams(padded_tokens))\n",
        "\n",
        "    # Add bigrams and unigrams to corpus\n",
        "    corpus_bigrams.extend(bigram_tokens)\n",
        "    corpus_unigrams.extend(padded_tokens)\n",
        "\n",
        "# Count occurrences of each bigram and unigram\n",
        "bigram_counts = Counter(corpus_bigrams)\n",
        "unigram_counts = Counter(corpus_unigrams)\n",
        "\n",
        "# Tokenize the sentence to calculate its probability\n",
        "sentence = \"I read a book by Danielle\"\n",
        "tokens = nltk.tokenize.word_tokenize(sentence.lower())\n",
        "padded_tokens = ['<s>'] + tokens + ['</s>']\n",
        "sentence_bigrams = list(bigrams(padded_tokens))\n",
        "\n",
        "# Calculate sentence probability\n",
        "sentence_probability = 1.0\n",
        "for bigram in sentence_bigrams:\n",
        "    preceding_word_count = unigram_counts[bigram[0]]  # Count of the preceding word in the bigram\n",
        "    if preceding_word_count > 0:\n",
        "        bigram_probability = bigram_counts[bigram] / preceding_word_count\n",
        "        sentence_probability *= bigram_probability\n",
        "    else:\n",
        "        # If the preceding word count is 0, set the sentence probability to 0\n",
        "        sentence_probability = 0.0\n",
        "        break\n",
        "\n",
        "print(\"Sentence Probability:\", sentence_probability)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq0O4xnjBjrT",
        "outputId": "3a2b0e1d-7b07-4781-e5b4-f7adbdd7e89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Probability: 0.07407407407407407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smoothed Bigram Model"
      ],
      "metadata": {
        "id": "09Ql0ZqmifC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import bigrams\n",
        "from collections import Counter\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"He read a book\",\n",
        "    \"I read a different book\",\n",
        "    \"He read a book by Danielle\"\n",
        "]\n",
        "\n",
        "# Tokenize each sentence and generate bigrams\n",
        "corpus_bigrams = []\n",
        "corpus_unigrams = []\n",
        "for sentence in sentences:\n",
        "    # Tokenize the sentence\n",
        "    tokens = nltk.tokenize.word_tokenize(sentence.lower())\n",
        "\n",
        "    # Add padding to the tokens\n",
        "    padded_tokens = ['<s>'] + tokens + ['</s>']\n",
        "\n",
        "    # Generate bigrams\n",
        "    bigram_tokens = list(bigrams(padded_tokens))\n",
        "\n",
        "    # Add bigrams and unigrams to corpus\n",
        "    corpus_bigrams.extend(bigram_tokens)\n",
        "    corpus_unigrams.extend(padded_tokens)\n",
        "\n",
        "# Count occurrences of each bigram and unigram\n",
        "bigram_counts = Counter(corpus_bigrams)\n",
        "unigram_counts = Counter(corpus_unigrams)\n",
        "\n",
        "# Calculate vocabulary size\n",
        "vocabulary_size = len(set(corpus_unigrams))\n",
        "\n",
        "# Calculate smoothed bigram probabilities\n",
        "smoothed_bigram_probabilities = {}\n",
        "for bigram in bigram_counts:\n",
        "    preceding_word_count = unigram_counts[bigram[0]]  # Count of the preceding word in the bigram\n",
        "    smoothed_probability = (bigram_counts[bigram] + 1) / (preceding_word_count + vocabulary_size)\n",
        "    smoothed_bigram_probabilities[bigram] = smoothed_probability\n",
        "\n",
        "# Tokenize the sentence to calculate its probability\n",
        "sentence = \"I read a book by Danielle\"\n",
        "tokens = nltk.tokenize.word_tokenize(sentence.lower())\n",
        "padded_tokens = ['<s>'] + tokens + ['</s>']\n",
        "sentence_bigrams = list(bigrams(padded_tokens))\n",
        "\n",
        "# Calculate sentence probability using the smoothed bigram model\n",
        "sentence_probability = 1.0\n",
        "for bigram in sentence_bigrams:\n",
        "    if bigram in smoothed_bigram_probabilities:\n",
        "        bigram_probability = smoothed_bigram_probabilities[bigram]\n",
        "        sentence_probability *= bigram_probability\n",
        "    else:\n",
        "        # If the bigram is unseen, apply Laplace smoothing with a count of 1\n",
        "        preceding_word_count = unigram_counts[bigram[0]]\n",
        "        smoothed_probability = 1 / (preceding_word_count + vocabulary_size)\n",
        "        sentence_probability *= smoothed_probability\n",
        "\n",
        "print(\"Sentence Probability:\", sentence_probability)\n"
      ],
      "metadata": {
        "id": "aSq9OZg1qcjz",
        "outputId": "c598091d-9562-4de5-c4dc-c18ad3a06240",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Probability: 1.0101357919757919e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_probability(corpus, sentence, smoothing=False):\n",
        "  \"\"\"\n",
        "  Calculates the probability of a sentence using bigram language model.\n",
        "\n",
        "  Args:\n",
        "    corpus: A list of strings representing the training corpus.\n",
        "    sentence: The sentence for which to calculate the probability.\n",
        "    smoothing: Whether to use smoothing (Add-1 smoothing)\n",
        "\n",
        "  Returns:\n",
        "    The probability of the sentence.\n",
        "  \"\"\"\n",
        "  words = sentence.split()\n",
        "  if len(words) == 0:\n",
        "    return 0\n",
        "\n",
        "  # Calculate bigram counts\n",
        "  bigram_counts = {}\n",
        "  unigram_counts = {}\n",
        "  for line in corpus:\n",
        "    line_words = line.split()\n",
        "    for i in range(len(line_words) - 1):\n",
        "      bigram = (line_words[i], line_words[i+1])\n",
        "      unigram = line_words[i]\n",
        "      if bigram in bigram_counts:\n",
        "        bigram_counts[bigram] += 1\n",
        "      else:\n",
        "        bigram_counts[bigram] = 1\n",
        "      if unigram in unigram_counts:\n",
        "        unigram_counts[unigram] += 1\n",
        "      else:\n",
        "        unigram_counts[unigram] = 1\n",
        "\n",
        "  # Calculate sentence probability\n",
        "  probability = 1.0\n",
        "  for i in range(1, len(words)):\n",
        "    bigram = (words[i-1], words[i])\n",
        "    if smoothing:\n",
        "      # Add-1 smoothing\n",
        "      count = bigram_counts.get(bigram, 0) + 1\n",
        "      denominator = unigram_counts.get(words[i-1], 0) + len(unigram_counts)\n",
        "    else:\n",
        "      count = bigram_counts.get(bigram, 0)\n",
        "      denominator = unigram_counts.get(words[i-1], 0)\n",
        "    if denominator == 0:\n",
        "      probability = 0\n",
        "      break\n",
        "    probability *= (count / denominator)\n",
        "\n",
        "  return probability\n",
        "\n",
        "# Training corpus\n",
        "corpus = [\n",
        "  \"He read a book\",\n",
        "  \"I read a different book\",\n",
        "  \"He read a book by Danielle\",\n",
        "]\n",
        "\n",
        "# Sentence to evaluate\n",
        "sentence = \"I read a book by Danielle\"\n",
        "\n",
        "# Unsmoothed bigram probability\n",
        "unsmoothed_probability = calculate_sentence_probability(corpus, sentence)\n",
        "print(\"Unsmoothed bigram probability:\", unsmoothed_probability)\n",
        "\n",
        "# Smoothed bigram probability\n",
        "smoothed_probability = calculate_sentence_probability(corpus, sentence, smoothing=True)\n",
        "print(\"Smoothed bigram probability:\", smoothed_probability)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUDsgF8Erdhw",
        "outputId": "272e26f9-4ced-40b0-9742-9098ece5e59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsmoothed bigram probability: 0.6666666666666666\n",
            "Smoothed bigram probability: 0.001875\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}